{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_classification_colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmatdarmawan4/Tugas/blob/main/fix3_multiclass_classification_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMNL5-jmJHXY"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80bh__Ff3Mx"
      },
      "source": [
        "# Your Code Goes Here\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine_bunch = datasets.load_wine()\n",
        "wine_bunch.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVMkdqVjUBqU"
      },
      "source": [
        "wine_bunch['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzf1QOgmSoU6"
      },
      "source": [
        "\n",
        "FEATURES = wine_bunch['feature_names']\n",
        "TARGET = 'species'\n",
        "\n",
        "wine_df = pd.DataFrame(wine_bunch['data'], columns=FEATURES)\n",
        "wine_df[TARGET] = wine_bunch['target']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WaJ3dLRSzbF"
      },
      "source": [
        "wine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ANi2508zesk"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine_df[FEATURES],\n",
        "    wine_df[TARGET],\n",
        "    test_size=0.1,\n",
        "    random_state=45)\n",
        "\n",
        "y_train.groupby(y_train).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF4PhwSt1gDf"
      },
      "source": [
        "y_test.groupby(y_test).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBfw4yeH1qeB"
      },
      "source": [
        "estimator = SGDClassifier()\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=62\n",
        ")\n",
        "\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM5RsHnP1wm7"
      },
      "source": [
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gzMu4H816Au"
      },
      "source": [
        "X_validation = X_test\n",
        "y_validation = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aU3FSlN1_oV"
      },
      "source": [
        "wine_df[FEATURES].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_hp0u5f2ZPu"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(wine_df[FEATURES])\n",
        "\n",
        "pd.DataFrame(\n",
        "    scaler.transform(wine_df[FEATURES]),\n",
        "    columns=FEATURES\n",
        ").describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uguf8lCD2mZ1"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "estimator = Pipeline(\n",
        "  steps=[\n",
        "    ['scale', StandardScaler()],\n",
        "    ['classifier', SGDClassifier()],\n",
        "  ]\n",
        ")\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train[FEATURES],\n",
        "    y_train,\n",
        "    cv=62,\n",
        "    # We calculated F1 here too. This isn't required.\n",
        "    scoring=make_scorer(f1_score, average='micro')\n",
        ")\n",
        "\n",
        "# Solution Starts Here\n",
        "estimator.fit(X_train, y_train)\n",
        "predictions = estimator.predict(X_validation)\n",
        "f1_score(y_validation, predictions, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpUGdKkY3M_f"
      },
      "source": [
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(y_validation, predictions)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_validation, predictions)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_validation, predictions, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_validation, predictions, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_validation, predictions, average='weighted')))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(y_validation, predictions, target_names=['Class 1', 'Class 2', 'Class 3']))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}